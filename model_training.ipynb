{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ec7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're on Google Colab, use `!pip install -r requirements.txt` instead\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is being used\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"PyTorch using CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c834d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only if using COCO dataset, if using Flickr, the images are pulled directly \n",
    "!wget http://images.cocodataset.org/zips/train2014.zip\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip\n",
    "!wget http://images.cocodataset.org/zips/test2014.zip\n",
    "\n",
    "!unzip train2014.zip -d coco\n",
    "!unzip test2014.zip -d coco\n",
    "!unzip val2014.zip -d coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "#################\n",
    "# Configuration #\n",
    "#################\n",
    "SAVE_DIR = \"embedded_data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "COCO_ROOT = \"coco\"  # Local folder where COCO images are stored\n",
    "\n",
    "#####################\n",
    "# Dataset Selection #\n",
    "#####################\n",
    "\n",
    "# Switch between available datasets here, code supports both:\n",
    "# DATASET_NAME = \"jxie/flickr8k\"\n",
    "DATASET_NAME = \"yerevann/coco-karpathy\"\n",
    "SPLIT = \"validation+test+train\"\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(DATASET_NAME, split=SPLIT)\n",
    "print(f\"Loaded dataset '{DATASET_NAME}' with {len(ds)} entries\")\n",
    "print(f\"Dataset shape: {ds.shape}\")\n",
    "\n",
    "#############\n",
    "# Load CLIP #\n",
    "#############\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "clip = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "#######################\n",
    "# Build Captiom Model #\n",
    "#######################\n",
    "\n",
    "caption_map = {}\n",
    "for i, row in enumerate(ds):\n",
    "    if DATASET_NAME == \"jxie/flickr8k\":\n",
    "        # Flick8k: Each image has 5 caption columns\n",
    "        caption_map[i] = [row[f\"caption_{j}\"] for j in range(5)]\n",
    "    else:\n",
    "        # COCO: List of captions stored under 'sentences'\n",
    "        caption_map[i] = row[\"sentences\"]\n",
    "\n",
    "# Save captions to disk\n",
    "with open(os.path.join(SAVE_DIR, \"captions.json\"), \"w\") as f:\n",
    "    json.dump(caption_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_images(images):\n",
    "    ''' \n",
    "    Takes a batch of PIL images and produces CLIP vision embeddings (normalized without CLS token)\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array [batch, patches, embed_dim] in float16 CPU format\n",
    "        (to reduce RAM and disk usage)\n",
    "    '''\n",
    "    \n",
    "    # Preprocess images to CLIP format and move to device\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # Autocast (amp) allows CLIP to run heavy matrix ops in float16\n",
    "    # while keeping precision ops in float32. Benefits include faster\n",
    "    # inference and lower memory usage.\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = clip.vision_model(inputs[\"pixel_values\"], output_hidden_states=True)\n",
    "    \n",
    "    # Removes CLS token\n",
    "    # CLIP output: CLS token + patch embedding\n",
    "    # CLS = global summary vector (which would be used for classification)\n",
    "    # Not super helpful in our case as we want local visual features\n",
    "    patch_tokens = out.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    # Normalize embeddings for stability\n",
    "    tokens = torch.nn.functional.normalize(patch_tokens, p=2, dim=-1)\n",
    "\n",
    "    # Convert to float16, reduce RAM and disk by 50%\n",
    "    return tokens.half().cpu().numpy()\n",
    "\n",
    "def load_image(filename):\n",
    "    '''\n",
    "    Construct valid path to a COCO image based on its filename convention.\n",
    "\n",
    "    Example:\n",
    "    'COCO_val2014_000000184613.jpg'\n",
    "        -> folder = 'val2014'\n",
    "        -> full path = 'coco/val2014/...'\n",
    "\n",
    "    Raises a clear error if the file is missing,\n",
    "    preventing silent training corruption.\n",
    "    '''\n",
    "    folder = filename.split(\"_\")[1] \n",
    "    full_path = os.path.join(COCO_ROOT, folder, filename)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"COCO image not found: {full_path}\")\n",
    "    \n",
    "    return Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "#####################\n",
    "# Create Embeddings #\n",
    "#####################\n",
    "\n",
    "# NOTE:\n",
    "# COCO contains ~90k rows, and we were hitting limits on Colab\n",
    "# We only embed 1 out of 5 entries (~20% sampling)\n",
    "# If you want 100% coverage, set TOTAL = len(ds)\n",
    "TOTAL = len(ds)//5 if DATASET_NAME == \"yerevann/coco-karpathy\" else len(ds)\n",
    "print(f\"Unique images selected for embedding: {TOTAL:,}\")\n",
    "\n",
    "# Determine the embedding shape by running one test forward pass\n",
    "test_img = load_image(ds[0]['filename']) if DATASET_NAME != \"jxie/flickr8k\" else ds[0][\"image\"]\n",
    "test_emb = embed_images([test_img])\n",
    "TOKEN_SHAPE = test_emb.shape[1:]\n",
    "\n",
    "print(f\"Embedding shape per image: {TOKEN_SHAPE}\")\n",
    "\n",
    "# Create an on-disk array to store embeddings continuously instead of once at the end\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "# Prepare memory-mapped array for incremental saving\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, \"patch_embeds_float16.npy\")\n",
    "\n",
    "fp = open_memmap(\n",
    "    SAVE_PATH,\n",
    "    mode=\"w+\",\n",
    "    dtype=np.float16,\n",
    "    shape=(TOTAL,) + TOKEN_SHAPE,\n",
    ")\n",
    "\n",
    "print(f\"Preallocated memmap on disk: {SAVE_PATH}\")\n",
    "print(f\"Full shape: {(TOTAL,) + TOKEN_SHAPE}\\n\")\n",
    "print(\"Starting embedding process...\\n\")\n",
    "\n",
    "idx = 0\n",
    "for start in trange(0, TOTAL, BATCH_SIZE):\n",
    "    \n",
    "    end = min(start + BATCH_SIZE, TOTAL)\n",
    "    \n",
    "    batch_rows = ds[start:end]\n",
    "\n",
    "    # Load image files for the current batch\n",
    "    if DATASET_NAME == \"jxie/flickr8k\":\n",
    "        batch = batch_rows[\"image\"]  # PIL Images already present\n",
    "    else:\n",
    "        # COCO-Karpathy: use filename column name\n",
    "        batch = [load_image(row) for row in batch_rows['filename']]\n",
    "\n",
    "    # Embed via CLIP\n",
    "    embeds = embed_images(batch)\n",
    "    b = embeds.shape[0]\n",
    "\n",
    "    # Write batch directly to disk\n",
    "    fp[idx:idx + b] = embeds\n",
    "    idx += b\n",
    "\n",
    "# Flush and release memmap\n",
    "del fp\n",
    "\n",
    "print(f\"Embedding complete: saved to: {SAVE_PATH}\")\n",
    "print(\"Output Folder:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0870e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "##############################\n",
    "# Load Embeddings + Captions #\n",
    "##############################\n",
    "\n",
    "SAVE_DIR = \"embedded_images\"\n",
    "BART_MODEL_ID = \"sshleifer/distilbart-cnn-12-6\"\n",
    "MAX_LEN = 64 # Max caption length (tokenized)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading embeddings...\")\n",
    "embs = np.load(SAVE_PATH, mmap_mode=\"r\")\n",
    "\n",
    "print(\"Loading captions...\")\n",
    "with open(f\"{SAVE_DIR}/captions.json\",\"r\") as f:\n",
    "    caption_map = json.load(f)\n",
    "\n",
    "all_imgs = np.arange(len(embs))\n",
    "\n",
    "# First split: 80% train, 20% val+test\n",
    "train_imgs, tmp = train_test_split(all_imgs, test_size=0.2, random_state=42)\n",
    "# Second split: split the reserved 20% in half: 10% val, 10% test\n",
    "val_imgs, test_imgs = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Each image has multiple corresponding captions\n",
    "# We expand the ds by duplicating image embeds per caption (idx only)\n",
    "def expand_ids(imgs):\n",
    "    img_idx, caps = [], []\n",
    "    for i in imgs:\n",
    "        for c in caption_map[str(i)]:\n",
    "            img_idx.append(i)\n",
    "            caps.append(c)\n",
    "    return img_idx, caps\n",
    "\n",
    "train_img_idx, train_caps = expand_ids(train_imgs)\n",
    "val_img_idx, val_caps = expand_ids(val_imgs)\n",
    "test_img_idx, test_caps = expand_ids(test_imgs)\n",
    "\n",
    "print(\"Expanded Caption Instances (Image duplicated per caption):\")\n",
    "print(f\"Train captions: {len(train_caps):,}\")\n",
    "print(f\"Val captions:   {len(val_caps):,}\")\n",
    "print(f\"Test captions:  {len(test_caps):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a110892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for DistilBART\n",
    "tokenizer = AutoTokenizer.from_pretrained(BART_MODEL_ID)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    '''\n",
    "    Dataset that pairs\n",
    "        - Precomputed CLIP patch embeddings\n",
    "        - Tokenized caption text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_idx, captions, embeddings, tokenizer):\n",
    "        self.img_idx = img_idx          # image index per caption\n",
    "        self.captions = captions        # raw text list\n",
    "        self.embeddings = embeddings    # memmapped numpy array of CLIP embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_ids = img_idx        # used for eval stats\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Select image ID for this caption\n",
    "        idx = self.img_idx[i]\n",
    "        txt = self.captions[i]\n",
    "\n",
    "        # Load embedding for corresponding image\n",
    "        vis = torch.from_numpy(self.embeddings[idx]).to(torch.float16)\n",
    "\n",
    "        # Tokenize caption text for language modeling\n",
    "        # - pad/truncate to fixed sequence length\n",
    "        # - return PyTorch tensor\n",
    "        tok = self.tokenizer(\n",
    "            txt,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "         # Return model-ready training batch dictionary\n",
    "        return {\n",
    "            \"patch_embeds\": vis,                         # (patch_tokens, embed_dim)\n",
    "            \"labels\": tok[\"input_ids\"].squeeze(0),       # remove batch dimension\n",
    "            \"image_id\": idx                              # kept for BLEU/CIDEr eval\n",
    "        }\n",
    "\n",
    "train_ds = ImageDataset(train_img_idx, train_caps, embs, tokenizer)\n",
    "val_ds = ImageDataset(val_img_idx, val_caps, embs, tokenizer)\n",
    "test_ds = ImageDataset(test_img_idx, test_caps, embs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CLIP2DistilBART(nn.Module):\n",
    "    '''\n",
    "    A lightweight multimodal captioning model that:\n",
    "\n",
    "    1. Accepts CLIP patch embeddings as input\n",
    "    2. Projects them through an adapter layer \n",
    "        - match CLIP dimension to BART encoder dimensions\n",
    "    3. Feeds them into DistilBART\n",
    "        - encoder receives adapted visual patches\n",
    "        - decoder generates captions text\n",
    "    \n",
    "    Efficient as only small adapter and BART is fine-tuned\n",
    "    '''\n",
    "\n",
    "    _keys_to_ignore_on_save = [] # Prevents warnings during saving\n",
    "\n",
    "    def __init__(self, bart_id, embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained DistilBART language model (decoder)\n",
    "        self.bart = AutoModelForSeq2SeqLM.from_pretrained(bart_id)\n",
    "\n",
    "        # Linear adapter: map CLIP embeddings -> BART encoder dimension\n",
    "        # Shape: (num_patches, embed_dim) → (num_patches, bart_dim)\n",
    "        self.adapter = nn.Linear(embed_dim, self.bart.config.d_model)\n",
    "\n",
    "    def forward(self, patch_embeds, labels=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            patch_embeds: (batch, patch_tokens, CLIP_dim)\n",
    "            labels: tokenized caption IDs (optional)\n",
    "                    If provided → training mode (returns loss)\n",
    "                    If None → inference mode (decoder free-runs)\n",
    "\n",
    "        Returns:\n",
    "            Standard Seq2SeqLMOutput from DistilBART\n",
    "        '''\n",
    "\n",
    "        # Convert to float32 inside adapter for numerical stability\n",
    "        enc = self.adapter(patch_embeds.float())\n",
    "        # Wrap as valid BART encoder output\n",
    "        enc_out = BaseModelOutput(last_hidden_state=enc)\n",
    "\n",
    "        # Forward through BART\n",
    "        return self.bart(\n",
    "            encoder_outputs=enc_out,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85782439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_SAFE_TENSORS\"] = \"1\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BART_MODEL_ID = \"sshleifer/distilbart-cnn-12-6\"\n",
    "embed_dim = embs.shape[-1]\n",
    "\n",
    "model = CLIP2DistilBART(BART_MODEL_ID, embed_dim).to(DEVICE)\n",
    "\n",
    "def collate(batch):\n",
    "    vis = torch.stack([b[\"patch_embeds\"] for b in batch])\n",
    "    lbl = torch.stack([b[\"labels\"] for b in batch])\n",
    "    return {\"patch_embeds\": vis, \"labels\": lbl}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./caption_model\",\n",
    "    per_device_train_batch_size=128,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=30,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "class WrappedTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        output_dir = output_dir or self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.model.bart.save_pretrained(output_dir, safe_serialization=False)\n",
    "        torch.save(self.model.adapter.state_dict(), f\"{output_dir}/adapter.pt\")\n",
    "        torch.save({\n",
    "            \"adapter_in\": self.model.adapter.in_features,\n",
    "            \"adapter_out\": self.model.adapter.out_features,\n",
    "            \"bart_model\": BART_MODEL_ID,\n",
    "        }, f\"{output_dir}/config.pt\")\n",
    "\n",
    "trainer = WrappedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./caption_model\")\n",
    "tokenizer.save_pretrained(\"./caption_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "preds = {}\n",
    "refs = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(test_ds))):\n",
    "    sample = test_ds[i]\n",
    "    vis = sample[\"patch_embeds\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    enc = model.adapter(vis.float())\n",
    "    enc_out = BaseModelOutput(last_hidden_state=enc)\n",
    "\n",
    "    out = model.bart.generate(\n",
    "        encoder_outputs=enc_out,\n",
    "        max_length=16,\n",
    "        num_beams=5,\n",
    "    )\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    preds[i] = [pred]\n",
    "\n",
    "    imgid = sample[\"image_id\"]  \n",
    "    refs[i] = caption_map[str(imgid)]\n",
    "\n",
    "bleu, _ = Bleu(4).compute_score(refs, preds)\n",
    "cider, _ = Cider().compute_score(refs, preds)\n",
    "\n",
    "print(\"BLEU-4:\", bleu[3])\n",
    "print(\"CIDEr:\", cider)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
